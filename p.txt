1. Write a program to cluster a set of points using K-means. Consider, K=4, clusters. Consider Euclidean distance as the distance measure. Randomly initialize a cluster mean as one of the data points. Iterate for 10 iterations. After iterations are over, print the final cluster means for each of the clusters.
Data set iris data set
->
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
import random

# Load the iris dataset
iris = load_iris()
data = iris['data']

# Number of clusters
K = 4

# Randomly initialize the centroids (cluster means) by selecting K random points from the dataset
np.random.seed(42)
initial_centroids = data[np.random.choice(data.shape[0], K, replace=False)]

# Function to calculate Euclidean distance
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# Function to assign each point to the nearest cluster
def assign_clusters(data, centroids):
    clusters = []
    for point in data:
        distances = [euclidean_distance(point, centroid) for centroid in centroids]
        clusters.append(np.argmin(distances))
    return np.array(clusters)

# Function to update centroids based on cluster assignment
def update_centroids(data, clusters, K):
    new_centroids = np.zeros((K, data.shape[1]))
    for k in range(K):
        cluster_points = data[clusters == k]
        if len(cluster_points) > 0:
            new_centroids[k] = np.mean(cluster_points, axis=0)
    return new_centroids

# Perform K-means for 10 iterations
centroids = initial_centroids
for i in range(10):
    clusters = assign_clusters(data, centroids)
    centroids = update_centroids(data, clusters, K)

# Print the final cluster means
print("Final cluster means (centroids) after 10 iterations:")
for idx, centroid in enumerate(centroids):
    print(f"Cluster {idx+1} mean: {centroid}")


o/p
Final cluster means (centroids) after 10 iterations:
Cluster 1 mean: [6.29361702 2.9        4.95106383 1.72978723]
Cluster 2 mean: [5.006 3.428 1.462 0.246]
Cluster 3 mean: [7.08695652 3.12608696 6.01304348 2.14347826]
Cluster 4 mean: [5.58       2.63333333 3.98666667 1.23333333]
------------------------------------------------------------------------------------------------

2. The table below gives the amount of Crabby Cakes made by Bakers for each year heâ€™s worked.
Graph the data on a scatter plot, find the line of best fit, and write the equation for the line you draw.
Years worked
1
2
3
4
5
6
Cakes made
6,500
7,805
10,835
11,230
15,870
16,387
Correlation Coefficient (r): _________
Type of Correlation: ______________________
Using the linear regression equation predict how many Crabby cakes he will make after working 10 years.
->
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Data for years worked and cakes made
years_worked = np.array([1, 2, 3, 4, 5, 6])
cakes_made = np.array([6500, 7805, 10835, 11230, 15870, 16387])

# Create a scatter plot
plt.scatter(years_worked, cakes_made, color='blue', label='Cakes made')

# Perform linear regression
slope, intercept, r_value, p_value, std_err = stats.linregress(years_worked, cakes_made)

# Line of best fit
line_of_best_fit = slope * years_worked + intercept

# Plot the line of best fit
plt.plot(years_worked, line_of_best_fit, color='red', label=f'Line of best fit')

# Add labels and title
plt.xlabel('Years worked')
plt.ylabel('Cakes made')
plt.title('Crabby Cakes made by Baker')
plt.legend()

# Show the plot
plt.show()

# Correlation coefficient (r)
correlation_coefficient = r_value

# Predict the number of cakes made after 10 years using the linear regression equation
predicted_cakes_after_10_years = slope * 10 + intercept

correlation_coefficient, predicted_cakes_after_10_years

o/p
will be a graph
---------------------------------------------------------------------------------------------

3. Write a program for multiple regression for predicting stock_index_price (dependent Variable) using two independent variables (interest_rate and unemployment_rate). Refer Stock_data file
Predict Stock index price for interest rate = 3 and unemployment rate is 5.7
->
import pandas as pd
from sklearn.linear_model import LinearRegression

# Load the stock data (assuming the file is in CSV format)
# Replace 'Stock_data.csv' with the correct file path
stock_data = pd.read_csv('Stock_data.csv')

# Define the independent variables (interest_rate, unemployment_rate) and dependent variable (stock_index_price)
X = stock_data[['interest_rate', 'unemployment_rate']]  # Independent variables
y = stock_data['stock_index_price']  # Dependent variable

# Create the linear regression model
model = LinearRegression()

# Fit the model to the data
model.fit(X, y)

# Define the input for prediction (interest rate = 3 and unemployment rate = 5.7)
interest_rate = 3
unemployment_rate = 5.7
predicted_stock_index_price = model.predict([[interest_rate, unemployment_rate]])

# Print the predicted stock index price
print(f"Predicted stock index price: {predicted_stock_index_price[0]}")

o/p
Predicted stock index price: 1409.1463642013675

-------------------------------------------------------------------------------------------------


4. Write a multiple regression program for predicting CO2 emission level when Volume 1300 of Car is and Engine weight is 3300Kg. Refer Car data file.
->
import pandas as pd
from sklearn.linear_model import LinearRegression

# Load the car data (assuming the file is in CSV format)
# Replace 'Car_data.csv' with the correct file path
car_data = pd.read_csv('Car_data.csv')

# Define the independent variables (Volume, Engine weight) and dependent variable (CO2 emission level)
X = car_data[['Volume', 'Weight']]  # Independent variables: Volume and Engine weight
y = car_data['CO2']  # Dependent variable: CO2 emission level

# Create the linear regression model
model = LinearRegression()

# Fit the model to the data
model.fit(X, y)

# Define the input for prediction (Volume = 1300 and Engine weight = 3300 Kg)
volume = 1300
engine_weight = 3300
predicted_CO2 = model.predict([[volume, engine_weight]])

# Print the predicted CO2 emission level
print(f"Predicted CO2 emission level: {predicted_CO2[0]} g/km")

o/p
Predicted CO2 emission level: 114.7596800692229 g/km

----------------------------------------------------------------------------------------------


5. Write a logistic regression program for predicting customer will subscribe for term deposit Refer Bank data set 1
->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the bank data (assuming the file is in CSV format)
# Replace 'Bank_data_set_1.csv' with the correct file path
bank_data = pd.read_csv('Bank_data_set_1.csv')

# Data preprocessing
# Assuming the target column is 'subscribed' and features are columns like 'age', 'balance', 'duration', etc.
# You need to adjust based on the actual dataset structure
# Convert categorical data to dummy/indicator variables
bank_data = pd.get_dummies(bank_data, drop_first=True)

# Define the independent variables (features) and dependent variable (target: subscribed)
X = bank_data.drop('subscribed', axis=1)  # Features
y = bank_data['subscribed']  # Target variable

# Split the dataset into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create the logistic regression model
model = LogisticRegression(max_iter=1000)

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)

o/p
err

------------------------------------------------------------------------------------------------


6. Write a logistic regression program for predicting customer will subscribe for term deposit Refer Bank data set 2
->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the bank data (assuming the file is in CSV format)
# Replace 'Bank_data_set_2.csv' with the correct file path
bank_data = pd.read_csv('Bank_data_set_2.csv')

# Data preprocessing
# Assuming the target column is 'subscribed' and features include columns like 'age', 'balance', 'duration', etc.
# You need to adjust based on the actual dataset structure
# Convert categorical data to dummy/indicator variables
bank_data = pd.get_dummies(bank_data, drop_first=True)

# Define the independent variables (features) and dependent variable (target: subscribed)
X = bank_data.drop('subscribed', axis=1)  # Features (independent variables)
y = bank_data['subscribed']  # Target variable (dependent variable)

# Split the dataset into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create the logistic regression model
model = LogisticRegression(max_iter=1000)

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)

o/p
err

------------------------------------------------------------------------------------------------------------


7. Write a program for K means clustering model based on countries Longitude and Latitude data set 1
->
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Load the dataset (assuming the file is in CSV format)
# Replace 'Countries_data_set_1.csv' with the correct file path
data = pd.read_csv('Countries_data_set_1.csv')

# Assuming the dataset has columns named 'Longitude' and 'Latitude'
X = data[['Longitude', 'Latitude']]

# Create the K-Means model with a predefined number of clusters (K)
K = 3  # You can adjust the number of clusters based on your requirement
kmeans = KMeans(n_clusters=K)

# Fit the model to the data
kmeans.fit(X)

# Get the cluster labels
labels = kmeans.labels_

# Add the cluster labels to the original data for analysis
data['Cluster'] = labels

# Plot the clusters
plt.figure(figsize=(10, 6))
plt.scatter(data['Longitude'], data['Latitude'], c=data['Cluster'], cmap='rainbow', s=50)
plt.title(f"K-Means Clustering of Countries Based on Longitude and Latitude (K={K})")
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Plot the cluster centroids
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')

plt.legend()
plt.show()

# Print the resulting clusters and centroids
print("Cluster Centers (Centroids):")
print(centroids)

o/p
graph showing results

---------------------------------------------------------------------------------------------


8. Write a program for K means clustering model based on countries Longitude and Latitude data set 2
->
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Load the dataset (assuming the file is in CSV format)
# Replace 'Countries_data_set_2.csv' with the correct file path
data = pd.read_csv('Countries_data_set_2.csv')

# Assuming the dataset has columns named 'Longitude' and 'Latitude'
X = data[['Longitude', 'Latitude']]

# Create the K-Means model with a predefined number of clusters (K)
K = 4  # You can adjust the number of clusters based on your requirement
kmeans = KMeans(n_clusters=K)

# Fit the model to the data
kmeans.fit(X)

# Get the cluster labels
labels = kmeans.labels_

# Add the cluster labels to the original data for analysis
data['Cluster'] = labels

# Plot the clusters
plt.figure(figsize=(10, 6))
plt.scatter(data['Longitude'], data['Latitude'], c=data['Cluster'], cmap='rainbow', s=50)
plt.title(f"K-Means Clustering of Countries Based on Longitude and Latitude (K={K})")
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Plot the cluster centroids
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')

plt.legend()
plt.show()

# Print the resulting clusters and centroids
print("Cluster Centers (Centroids):")
print(centroids)

o/p
graph showing result

------------------------------------------------------------------------------------------------------


9. Write a program to classify iris data set using Random forest method
->
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features (sepal length, sepal width, petal length, petal width)
y = iris.target  # Target (species)

# Split the dataset into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create the Random Forest classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=iris.target_names)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)

o/p
Accuracy: 100.00%

Classification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45

--------------------------------------------------------------------------------------


10. Write a program to classify the social network Advertise data1 using Bayâ€™s classification method.
->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset (assuming the file is in CSV format)
# Replace 'Social_Network_Advertise_Data1.csv' with the correct file path
data = pd.read_csv('Social_Network_Advertise_Data1.csv')

# Display the first few rows of the dataset
print(data.head())

# Assuming the dataset has columns 'Age', 'EstimatedSalary' as features and 'Purchased' as the target
# Define independent variables (features) and dependent variable (target)
X = data[['Age', 'EstimatedSalary']]  # Features (independent variables)
y = data['Purchased']  # Target variable (dependent variable, 0 or 1 indicating purchase decision)

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (important for Naive Bayes since it doesn't assume a particular scale for features)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create the Gaussian Naive Bayes model
model = GaussianNB()

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)


o/p
User ID  Gender  Age  EstimatedSalary  Purchased
0  15624510    Male   19            19000          0
1  15810944    Male   35            20000          0
2  15668575  Female   26            43000          0
3  15603246  Female   27            57000          0
4  15804002    Male   19            76000          0
Accuracy: 93.75%

Classification Report:
              precision    recall  f1-score   support

           0       0.94      0.96      0.95        52
           1       0.93      0.89      0.91        28

    accuracy                           0.94        80
   macro avg       0.93      0.93      0.93        80
weighted avg       0.94      0.94      0.94        80

----------------------------------------------------------------------------------------


11. Write a program to classify the social network Advertise data2 using Bayâ€™s classification method
->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset (assuming the file is in CSV format)
# Replace 'Social_Network_Advertise_Data2.csv' with the correct file path
data = pd.read_csv('Social_Network_Advertise_Data2.csv')

# Display the first few rows of the dataset
print(data.head())

# Assuming the dataset has columns 'Age', 'EstimatedSalary' as features and 'Purchased' as the target
# Define independent variables (features) and dependent variable (target)
X = data[['Age', 'EstimatedSalary']]  # Features (independent variables)
y = data['Purchased']  # Target variable (dependent variable, 0 or 1 indicating purchase decision)

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (important for Naive Bayes since it doesn't assume a particular scale for features)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create the Gaussian Naive Bayes model
model = GaussianNB()

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)


o/p
User ID  Gender  Age  EstimatedSalary  Purchased
0  15624510    Male   19            19000          0
1  15810944    Male   35            20000          0
2  15668575  Female   26            43000          0
3  15603246  Female   27            57000          0
4  15804002    Male   19            76000          0
Accuracy: 88.10%

Classification Report:
              precision    recall  f1-score   support

           0       0.92      0.94      0.93        36
           1       0.60      0.50      0.55         6

    accuracy                           0.88        42
   macro avg       0.76      0.72      0.74        42
weighted avg       0.87      0.88      0.88        42

-------------------------------------------------------------------------------------------


12. Demonstrate the SVM model using the social network Advertise data1
->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset (assuming the file is in CSV format)
# Replace 'Social_Network_Advertise_Data1.csv' with the correct file path
data = pd.read_csv('Social_Network_Advertise_Data1.csv')

# Display the first few rows of the dataset to understand its structure
print(data.head())

# Assuming the dataset has columns 'Age', 'EstimatedSalary' as features and 'Purchased' as the target
# Define independent variables (features) and dependent variable (target)
X = data[['Age', 'EstimatedSalary']]  # Features (independent variables)
y = data['Purchased']  # Target variable (dependent variable, 0 or 1 indicating purchase decision)

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (important for SVM, as it is sensitive to the scale of data)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create the Support Vector Machine (SVM) model with a linear kernel
svm_model = SVC(kernel='linear', random_state=42)

# Fit the model to the training data
svm_model.fit(X_train, y_train)

# Predict on the test data
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)


o/p
    User ID  Gender  Age  EstimatedSalary  Purchased
0  15624510    Male   19            19000          0
1  15810944    Male   35            20000          0
2  15668575  Female   26            43000          0
3  15603246  Female   27            57000          0
4  15804002    Male   19            76000          0
Accuracy: 86.25%

Classification Report:
              precision    recall  f1-score   support

           0       0.85      0.96      0.90        52
           1       0.90      0.68      0.78        28

    accuracy                           0.86        80
   macro avg       0.88      0.82      0.84        80
weighted avg       0.87      0.86      0.86        80

------------------------------------------------------------------------------------------------

13. Demonstrate the SVM model using the social network Advertise data2
->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset (assuming the file is in CSV format)
# Replace 'Social_Network_Advertise_Data2.csv' with the correct file path
data = pd.read_csv('Social_Network_Advertise_Data2.csv')

# Display the first few rows of the dataset to understand its structure
print(data.head())

# Assuming the dataset has columns 'Age', 'EstimatedSalary' as features and 'Purchased' as the target
# Define independent variables (features) and dependent variable (target)
X = data[['Age', 'EstimatedSalary']]  # Features (independent variables)
y = data['Purchased']  # Target variable (dependent variable, 0 or 1 indicating purchase decision)

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (important for SVM, as it is sensitive to the scale of data)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create the Support Vector Machine (SVM) model with a linear kernel
svm_model = SVC(kernel='linear', random_state=42)

# Fit the model to the training data
svm_model.fit(X_train, y_train)

# Predict on the test data
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print the accuracy and classification report
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(report)


o/p
    User ID  Gender  Age  EstimatedSalary  Purchased
0  15624510    Male   19            19000          0
1  15810944    Male   35            20000          0
2  15668575  Female   26            43000          0
3  15603246  Female   27            57000          0
4  15804002    Male   19            76000          0
Accuracy: 88.10%

Classification Report:
              precision    recall  f1-score   support

           0       0.92      0.94      0.93        36
           1       0.60      0.50      0.55         6

    accuracy                           0.88        42
   macro avg       0.76      0.72      0.74        42
weighted avg       0.87      0.88      0.88        42


